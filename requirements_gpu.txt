llama-cpp-python -C cmake.args="-DGGML_CUDA=on"
--extra-index-url https://download.pytorch.org/whl/cu124
torch==2.4.1
torchvision==0.19.1
transformers
pillow
thefuzz
opencv-python
fastapi
uvicorn
jinja2
python-multipart
aiofiles
packaging
ninja
flash-attn, --global-option="--no-build-isolation"